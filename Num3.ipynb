{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61966f51-c5bc-4f2f-8641-50d6498a75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0dbb487-f446-4e7e-855e-423d9b1ab46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio', 'transcription']\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "dataset = load_dataset(\"babs/openslr-yoruba\")\n",
    "\n",
    "#split dataset\n",
    "\n",
    "data_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "#access the train and test sets\n",
    "train_ds = data_split['train']\n",
    "test_ds = data_split['test']\n",
    "\n",
    "# Print dataset column names\n",
    "print(train_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489de808-d706-41b0-8503-4caf24e28c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "n_mfcc = 13 # number of mfcc features\n",
    "num_epochs = 10 #number of training epoch (will be used in the training loop)\n",
    "hidden_dim = 128 #LSTM hidden state dimensionality\n",
    "\n",
    "\n",
    "# Create a label dictionary that maps each unique label to an integer\n",
    "unique_labels = set([sample['transcription'] for sample in train_ds])\n",
    "label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Update output_dim based on the number of unique labels\n",
    "output_dim = len(label_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd668ee-a3da-411b-9661-5afdacc0ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding function\n",
    "def encode_label(label):\n",
    "    if label not in label_dict:\n",
    "        raise ValueError(f\"Label '{label}' not found in label_dict.\")\n",
    "    return torch.tensor(label_dict[label], dtype=torch.long)  # No need for []\n",
    "\n",
    "# Extract MFCC and convert\n",
    "def extract_mfcc(audio):\n",
    "    y = np.array(audio['audio']['array'])\n",
    "    sr = audio['audio']['sampling_rate']\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return torch.tensor(mfccs.T, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Define the LSTM model\n",
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.hidden2label(lstm_out[:, -1, :])\n",
    "        return nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c733c0-cd92-4e9d-a1b9-4e81056dd419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = AudioLSTM(input_dim=n_mfcc, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sample in train_ds:\n",
    "        inputs = extract_mfcc(sample)\n",
    "        target_label = sample['transcription']\n",
    "        targets = encode_label(target_label)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_function(output, targets.unsqueeze(0))  # Add batch dimension\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_ds)}\")\n",
    "\n",
    "# Test on one sample (optional)\n",
    "with torch.no_grad():\n",
    "    sample = test_ds[0]\n",
    "    inputs = extract_mfcc(sample)\n",
    "    output = model(inputs)\n",
    "    predicted_label_idx = output.argmax(dim=1).item()\n",
    "    predicted_label = list(label_dict.keys())[list(label_dict.values()).index(predicted_label_idx)]\n",
    "    print(f\"Predicted: {predicted_label}, True: {sample['transcription']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a214dc-726a-4aec-a03d-98e1dda82be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
