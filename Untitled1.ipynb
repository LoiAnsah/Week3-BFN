{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5280c8-5fbc-45be-bfc4-57b10e4b7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522563e8-097a-4391-9a47-327c12356f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio', 'transcription']\n",
      "Sample 1: MFCCs shape: (13, 649)\n",
      " Kájọlà ni ibùsọ̀ táa ma kọ́kọ́ kàn ka tó dé Káṣọpẹ́.\n",
      "Sample 2: MFCCs shape: (13, 289)\n",
      "Orí ẹni ní ń mọ àtilà ẹni.\n",
      "Sample 3: MFCCs shape: (13, 385)\n",
      "Lọ gbá ilẹ̀ àárín ilé àti ti ẹ̀yìnkùlé.\n",
      "Sample 4: MFCCs shape: (13, 209)\n",
      "Ẹran ọ̀sìn ni ewúrẹ́ àti adìyẹ.\n",
      "Sample 5: MFCCs shape: (13, 353)\n",
      "Mo tiẹ̀ ríi gbọ́ pé osù tó ń bọ̀ ni ọdún ìgúnu.\n",
      "Sample 6: MFCCs shape: (13, 273)\n",
      "Mo nífẹ̀ẹ́ láti máa jẹ edé.\n",
      "Sample 7: MFCCs shape: (13, 433)\n",
      "Àwọn tí wọ́n darí iná fún eré orí ìtàgé náà gbìyànjú.\n",
      "Sample 8: MFCCs shape: (13, 305)\n",
      " Alhaja fẹ́ràn láti máa jẹ búrẹ́dì Sẹ́nígà.\n",
      "Sample 9: MFCCs shape: (13, 321)\n",
      "Gèlè náà gbé ẹwà rẹ̀ jáde.\n",
      "Sample 10: MFCCs shape: (13, 497)\n",
      " Ilé ẹ̀kọ́ girama tó wà ní Jíbówú ni Akíntádé ti bẹ̀rẹ̀ iṣẹ́.\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "dataset = load_dataset(\"babs/openslr-yoruba\")\n",
    "\n",
    "#split dataset\n",
    "\n",
    "data_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "#access the train and test sets\n",
    "train_ds = data_split['train']\n",
    "test_ds = data_split['test']\n",
    "\n",
    "\n",
    "# Print dataset column names\n",
    "print(train_ds.column_names)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    #process audio sample\n",
    "    audio_sample = train_ds[i]\n",
    "\n",
    "    #extract audio data and sampling rate\n",
    "    audio_data = audio_sample['audio']['array']\n",
    "    sampling_rate = audio_sample['audio']['sampling_rate']\n",
    "\n",
    "    #ensure data is numpy array\n",
    "    audio_data = np.array(audio_data)\n",
    "\n",
    "    #extract mfcc features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sampling_rate, n_mfcc=13)\n",
    "\n",
    "    #display the mfcc features\n",
    "    print(f\"Sample {i+1}: MFCCs shape: {mfccs.shape}\")\n",
    "    print(train_ds['transcription'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa031615-0c17-48a4-8d56-aa1a2fd72e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "n_mfcc = 13 # number of mfcc features\n",
    "num_epochs = 10 #number of training epoch (will be used in the training loop)\n",
    "hidden_dim = 128 #LSTM hidden state dimensionality\n",
    "\n",
    "\n",
    "# Create a label dictionary that maps each unique label to an integer\n",
    "unique_labels = set([sample['transcription'] for sample in train_ds])\n",
    "label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Update output_dim based on the number of unique labels\n",
    "output_dim = len(label_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4527b128-b3e5-41f9-bddf-fd49c7d5ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_label(label):\n",
    "    if label not in label_dict:\n",
    "        raise ValueError(f\"Label '{label}' not found in label_dict.\")\n",
    "    return torch.tensor([label_dict[label]], dtype=torch.long)\n",
    "\n",
    "#extract MFCC and convert\n",
    "def extract_mfcc(audio):\n",
    "    y = np.array(audio['audio']['array'])\n",
    "    sr = audio['audio']['sampling_rate']\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Shape should be (n_mfcc, num_frames), transpose to (num_frames, n_mfcc)\n",
    "    return torch.tensor(mfccs.T, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Define the LSTM model\n",
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.hidden2label(lstm_out[:, -1, :])  # Use the output from the last time step\n",
    "        return nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e0bd51-4484-4821-97f7-280e48bb8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = AudioLSTM(input_dim=n_mfcc, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sample in train_ds:\n",
    "        inputs = extract_mfcc(sample)\n",
    "        target_label = sample['transcription']\n",
    "        targets = encode_label(target_label)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_ds)}\")\n",
    "\n",
    "# Test on one sample (optional)\n",
    "with torch.no_grad():\n",
    "    sample = test_ds[0]\n",
    "    inputs = extract_mfcc(sample)\n",
    "    output = model(inputs)\n",
    "    print(f\"Predicted: {output.argmax(dim=1).item()}, True: {sample['transcription']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada29de0-c640-419f-a5ba-7d82611e4d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
